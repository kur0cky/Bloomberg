---
title: "グラフィカルLassoことはじめ"
date: "`r Sys.Date()`"
author: Yutaka Kuroki
output:
  rmdformats::readthedown:
    code_folding: hide
    self_contained: true
    thumbnails: false
    lightbox: false
    md_extensions: -ascii_identifiers
---


```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
                 cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#はじめに
グラフィカルLassoの調査です。まだまだ良く分かっていないですがよろしくお願いいたします。

##モチベーション（グラフィカルモデル）
- 一般に、ノイジーな実データについて、何かを言うのは難しい
- 多変数間の関係について何か言いたい場合が結構多い
- 視覚的に確認できれば、直観的に理解できるし見栄えも派手

##モチベーション（スパースモデリング）
- 多変数間の関係性において、最も基本的なものは**分散共分散行列**である
- しかし、相関係数には問題が多い（交絡などのバイアス、ノイズなどによる信頼性等）
- 一般的に関係が**無いこと**の証明はいわゆる**悪魔の証明**である
- その辺をスパースにしていけたら理想的??(motivation)

##モチベーション(Graphical Lasso)
- 各変数に**無意味**なものが全くなさそうな時の**関係性**を見る時に使えそう。あとは異常検知で用いられているらしい
    - センサーデータ
    - 株価指数データ



#Graphical Lassoとは

motivation

- 基本的なところから拡張していくことでGraphical Lassoのありがたみを感じたい

##ガウス型グラフィカルモデル
変数同士の関係性を見るために、視覚的にわかりやすい**マルコフグラフ**を考える

- マルコフグラフ：ただの確率場を意識した無向グラフ。

また、ノイズの多い実データでは、脆弱性の観点からガウス型以外を考えるのは難しい（らしい。やってみないと何とも言えない）。

変数が$M$個、サンプルサイズ$N$個の場合を考える。与えられたデータを$M$次元ベクトル$\bf{x}$とすると、多次元正規分布の密度関数は以下のよう

\begin{equation}
N(\bf x | \bf \mu, \Lambda) = 
\dfrac{ (\text{det}\Lambda)^{1/2} }{ (2\pi)^{M/2} }
\text{exp}\left\{
-\dfrac{1}{2} (\bf x - \bf \mu)^{\text T} \Lambda(\bf x - \bf \mu)
\right\}

\end{equation}
- $\bf \mu$：$\bf x$の平均ベクトル
- $\Lambda$：精度行列
- $\bf{x}$ $= (x_1, x_2,\dots,x_N)^{\text T}$

次に、2変数$x_1, x_2$の関係だけを見るために、他の変数を与えた時の同次分布を考える。実データを扱う際には、すべての変数に関して平均0標準偏差偏差1に標準化するので$\bf\mu=0$を仮定する。

\begin{equation}
p(x_1,x_2 | x_3,\dots,x_N) \propto
  \text{exp} \left\{
    -\dfrac{1}{2}(\Lambda_{1,2}x_1^2 + 2\Lambda_{1,2}x_1x_2 + \Lambda_{2,2}x_2^2)
  \right\}
\end{equation}

ここで、$x_1,x_2$が条件付き独立であるとは、上式が$x_1$の部分と$x_2$の部分の積で書ける必要がある。

つまり
$$\Lambda_{1,2} = 0$$
なら、ガウス型グラフィカルモデルの枠組みでは$x_1$と$x_2$は無関係である。

- ガウス型を仮定すると、グラフィカルモデルの構造学習は精度行列の推定に帰着できる
- 関係性にスパース性を入れる$\rightarrow$ スパースな精度行列を推定する
- $\Lambda$の閾値以下を0にするだけではダメ
    - 正定値行列ではなくなってしまう
    - 多次元正規分布の$\text{exp}\{\}$の部分がおかしなことになり得る


##疎構造学習


まずはスパース性を考慮しない$\Lambda$の最尤推定を考える。標準化した$N$個のデータの、多次元正規分布を仮定した対数尤度は

\begin{equation}
\begin{split}
\text{ln}\prod_{n=1}^N N(\bf x|\bf 0 \Lambda^{-1}) = 
  \sum_{n=1}^N\left\{
    -\dfrac{M}{2}\text{ln}(2\pi) + \dfrac{1}{2}\text{ln}(\text{det}\Lambda) - \dfrac{1}{2}\bf x^{\text T}\Lambda \bf x
  \right\}\\
= \text{（定数）}-\dfrac{N}{2}\left\{
  \text{Tr}\left(\Lambda\dfrac{1}{N}\sum_{n=1}^N\bf x \bf x^{\text{T}}\right) - \text{ln}(\text{det}\Lambda)
\right\}\\
= \text{（定数）} - \dfrac{N}{2}\{\text{Tr}(\Lambda S) - \text{ln}(\text{det} \Lambda ) \}
\end{split}
\end{equation}

- $S$：分散共分散行列

で与えられる。尤度を最大にするためには$\{\text{Tr}(\Lambda S) - \text{ln}(\text{det} \Lambda ) \}$を最小化すればよい。

[Banerjee, Ghaoui and Natsoulis(2006)](https://core.ac.uk/display/21153650)

は、これにL1正則化項を加えた問題が凸最適化問題になっていることを示した。
\begin{equation}
\text{minimize}\qquad f(\Lambda) = \text{Tr}(\Lambda S) - \text{ln}(\text{det}\Lambda) + \rho||\Lambda||_1
\end{equation}

L1ノルムにより、普通に微分することは出来ないが、**劣勾配法**というものがある（力不足により未だ分かっておりません）

$$
\dfrac{\partial f}{\partial \Lambda} = \Lambda^{-1} - S - \rho\ \text{sign}(\Lambda)
$$

#Rによる実行
数学の話は難しかったので、とりあえずRで試してみる。Rのパッケージでは、`glasso`がスパースな精度行列推定を実装している

```{r library}
library('glasso') # グラフィカルLasso
library('igraph') # プロット
library('MASS') # mvrnorm
library('spaceExt') # glasso with EM
```



##構造が既知の場合

既知である標本共分散行列を与える。
```{r}
s <- matrix(c(10,1,5,4,1,10,2,6,5,2,10,3,4,6,3,10), 4)
s

```

罰則項パラメータ rho = 0 として`glasso()`を実行。精度行列を推定する。
また、ここでは、(1,3), (2,4)が0であることがあらかじめ分かっているとする（ブラックリストのようなもの）。

推定した精度行列は以下である
```{r}
# 罰則項パラメータrho = 0 とした
# zero:(1,3)と(2,4)が既知で0だとあらかじめ言う
glasso.result <- glasso(s, rho=0, zero=matrix(c(1,2,3,4), 2))

glasso.result$wi # 推定した精度行列
```

次に隣接行列を作成し、プロットを行う。
```{r}
# 隣接行列を作る
# 0.0001よりも大きいかどうかの真偽
adjacency <- abs(glasso.result$wi) > 1E-4
adjacency

# 対角成分に0代入する
# 自分自身へのエッジを消す意味
diag(adjacency) <- 0

# plot
adjacency.plot <- graph.adjacency(adjacency, mode='undirected')
plot(adjacency.plot)

```

##構造が未知の場合

次の真の分散共分散行列を与える（未知）。
```{r}
s <- matrix(c(1,0.2,0.666,0.2,1,0.1,0.666,0.1,1),3)
s
```

それに従う3次元正規乱数を100個発生させる。また、その乱数から標本分散共分散行列を計算する
```{r}
data <- mvrnorm(n=100, mu=c(1,1,1), Sigma=s)
data.cov <- cov(data)
data.cov
```

前項でやった作業を関数にまとめてしまう`glasso.with.rho()`
```{r}
glasso.with.rho <- function(x, rho) {
  glasso.result <- glasso(x, rho=rho)
  adjacency <- abs(glasso.result$wi) > 1E-4; diag(adjacency) <- 0
  adjacency.plot <- graph.adjacency(adjacency, mode='undirected')
  plot(adjacency.plot)
  glasso.result
}
```

rhoを様々かえてみる。だんだんとスパースになっていく様子がわかる。
```{r}
glasso.with.rho(data.cov, 0)$wi
glasso.with.rho(data.cov, 0.1)$wi
glasso.with.rho(data.cov, 0.3)$wi
glasso.with.rho(data.cov, 0.5)$wi
glasso.with.rho(data.cov, 1)$wi
```


最適なrhoをどのように見つければよいのか。bic?? cv??

##欠損値を埋める

パッケージ`spaceExt`では、EMアルゴリズムを使用した、欠損値が含まれていてもglassoできる`glasso.miss()`が実装されている。

前項と同じ分散共分散行列を使用する。
```{r}
s <- matrix(c(1,0.2,0.666,0.2,1,0.1,0.666,0.1,1), 3)
s
```


3変数250個のデータを作る。うしろ50個を欠損にする。

```{r}
# 平均1
# 分散共分散は先ほど作成したもの
# データを生成
data <- mvrnorm(n=250, mu=c(1,1,1), Sigma=s)

# 後ろの50個を欠損にする。後の検討のために保存
true.missing <- data[200:250,3]

# テストデータに使用するためNA??
data[200:250,3] <- NA
```



```{r}
glasso.miss.result <- glasso.miss(data, emIter=10, rho=.1)
glasso.miss.result
# 隣接行列の作成
adjacency <- abs(glasso.miss.result$wi) > 1E-4
# 自身へのパスは省略
diag(adjacency) <- 0
adjacency.plot <- graph.adjacency(adjacency, mode='undirected')
# plot
plot(adjacency.plot)
```

平均二乗誤差を計算
```{r}
imputed <- glasso.miss.result$Y.imputed[200:250,3]

# 二乗誤差
# テストデータ固定してる

mean( (true.missing - imputed)^2 )
```
発展させれば交差検証法できそう。